---
type:
  - Article
author:
  - Justin T. Pickett
journal:
  - Annual Review of Criminology
year: 2019
---

* **Creation date**: `= this.file.ctime`
* **Last modified date**: `= this.file.mtime`

## Metadata

* **Author(s)**: Justin T. Pickett
* **Title**: Public Opinion and Criminal Justice Policy: Theory and Research
* **Date of publication**: 2019
* **Journal**: Annual Review of Criminology
* **Volume**: 2
* **Issue**: 1
* **Pages**: 405-428
* **URL**: [https://doi.org/10.1146/annurev-criminol-011518-024826](https://doi.org/10.1146/annurev-criminol-011518-024826)
* **Tags**: #comps_exam, #cj_system #survey #public_opinion
* **PDF Attachments**:
  * [pickettPublicOpinionCriminal2019.pdf](zotero://open-pdf/library/items/TX97D6VI)

## Abstract

This article reviews evidence for the effects of public opinion on court decision-making, capital punishment policy and use, correctional expenditures, and incarceration rates. It also assesses evidence about the factors explaining changes over time in public support for punitive crime policies. Most of this evidence originates from outside of our discipline. I identify two reasons that criminologists have not made more progress toward understanding the opinion-policy relationship. One is an unfamiliarity with important theoretical and empirical developments in political science pertaining to public policy mood, parallel opinion change, majoritarian congruence, and dynamic representation. Another is our overreliance on cross-sectional studies and preoccupation with comparing support levels elicited with different questions (global versus specific) and under different conditions (uninformed versus informed). I show how the resultant findings have contributed to misunderstandings about the nature of public opinion and created a false summit in our analysis of the opinion-policy relationship.

## My notes

### Introduction

Does the public collectively decide that lawbreakers should suffer more when crime increases and then get our representatives to do so? Author argues the answer is yes.
  
Criminology, as a field, though has a hard time answering this question.
* We have not kept up with the latest research in political science.
* We also have relied too much on cross-sectional studies and have not examined longitudinal relationships between macro-level phenomena and individual public opinion.
	* This has led us astray when it comes to question wording. Cross-sectional analysis revealed that public support for punitive policies is variable depending on how the question is worded and how much detail is provided -> **important research** -> nevertheless this misses the point that longitudinally, people's opinions have remained fairly constant.

Criminologists are too obsessed with the idea that public punitiveness is a myth and that politicians' claims that the public wanted tougher crime policies were disingenuous.

### Learning from Political Science

Most people know very little about the extent or nature of crime, legal statutes or procedures, sentencing laws, or the administration of punishment.
  
When criminologists conduct cross-sectional analyses demonstrating how giving respondents more information, depending on the information, can make respondents more or less punitive -> we are, again, missing out on important longitudinal data.
  
* Most research indicates that people across *information strata* respond pretty similarly to the same broader social processes even as their baseline levels of support are pretty different.

#### How can an uninformed public respond intelligibly to anything?

* Much like criminologists, political scientists were shocked to find out how little people knew -> most did not know their senator or representative, answers in surveys were inconsistent or contradictory and highly sensitive to word choices.
  
* Nevertheless, evidence accumulated showing:
  
	* Uninformed individuals were still able to formulate policy opinions consistent with their interests using informational shortcuts.
	  
	* Despite great instability in individual attitude reports over time, most survey marginals showed great stability.
	  
	* Different models have been proposed to make sense of these trends:
	  
		* All individuals have a **real attitude** which due to measurement error and current levels of information can cause deviation from the true attitude.
		  
		* Two groups of respondents -> one group has **real** and stable opinions while the other group lack real opinions and give wildly varying answers.
		  
		* All respondents lack preformed attitudes but are considering conflicting considerations which are determined by their topic-specific knowledge and interest. If survey particulars (e.g., question wording) were held constant, attitude statements should fluctuate randomly around the stable long-term central tendency of relevant considerations held by respondents (surveys draw out certain considerations more than others depending on wording). There should be less fluctuation over time in attitude reports for knowledgeable and engaged respondents -> less subject to question wording effects and random noise which would cause changes in their responses due to whatever it was that was causing them in the moment to consider the policy in a positive, negative, or neutral light.
		  
* All these models propose that response instability occurs because some there is a probabilistic function which defines how people will respond to a specific policy -> random reporting errors will be largest for the inattentive and uninformed.
	  
* When aggregated over many, independent respondents, the errors should balance out as a signal will eventually be able to be discerned from the noise -> despite low levels of knowledge and attentiveness among most respondents.
	  
	* Even if people answered randomly (flipping a coin), aggregated responses will remain stable over time in expectation.

* Collective decisions are more accurate than individual choices as long as group members have a better than random chance of choosing correctly for themselves. In terms of public opinion, this means aggregate attitudes should be more perceptive and thoughtful than those of individuals.
  
* Net public opinion change occurs when: 1) the composition of the public changes, or 2) some members of the public (even a tiny minority) updates their attitudes simultaneously and systematically. 

#### Are trends in punitiveness similar across groups?

* An important implication of the above research is that when public opinion does change, there is usually a reason. Research has found that across a variety of domains (sex, race, education, income, religion, age, etc.) trend lines were pretty similar even as baseline levels of support might be very different.
  
* **Public policy mood** -> Wording effects are usually irrelevant when studying trends in identically worded questions. However, by focusing on a certain set of specific policy preferences, surveys ignore issue bundling. Aggregate policy preferences on diverse issues (and measured with differently worded questions) move together in similar ways suggesting they share a common cause often called the **public policy mood** which is a latent, macro, multidimensional construct comprising the public's generalized dispositions towards certain kinds of policies (e.g., harshness of formal social control).
  
	* Aggregate responses to individual survey questions about specific polices are imperfect indicators of this latent attitude structure. Measuring public mood requires aggregating over respondents and then over questions and issues over time. The most popular technique is the [[dyad ratio algorithm]].
	  
		* Although base-line levels of support for various criminal justice policies have been found to be quite varied, trends in their levels of support tended to be quite similar (parallel trends) suggesting they all reflected the same underlying, latent construct.

* Also, Americans tend to be exposed to the same mass media regardless of pre-existing knowledge levels or demographic group. This results in attitude trends that are responsive to changes in objective conditions (or at least as objective as reported by the media). #question -> is this still true today in the age of the internet and right-wing media?

#### Do public officials care more about trends in public opinion or the actual levels of support?

* **Democracy at work** -> Increases in crime rates caused the punitive turn in criminal justice by elevating public fear of crime and punitiveness.
	* Generally research has found that punitive policy existence and use are related to aggregate support for them.
	* Affects judges too (in states where they are elected).
	* Generalize punitiveness and the public policy mood also seem to drive incarceration rates and crime spending -> rational anticipation by elected officials rather than through electoral turnover -> mediates the effects of crime rates.
	* "... had punitive mood leveled off, much less wanted, two decades before it actually started declining... the USA would have had 20% fewer incarcerations... each year." #quote 

* **Social constructionism** -> Media and political elites manipulated public opinion for profit and political ends with little regard for actual trends in crime rates or drug usage. **Favored hypothesis of criminologists reflecting our professional ideology, but the author argues it is wrong.**

 1. Do public officials care about public opinion? Yes.
 2. How do public officials learn about public opinion? Since the 1950s, polls are the answer.
 3. What type of public opinion matters most to public officials? The answer is aggregate, generalized, and dynamic policy preferences i.e., **the public policy mood**. -> may ignore majority or even super-majority preferences when they're inconsistent with trends in public mood but attend to them very closely with they align with the trends. -> **policy popularity is secondary to mood trends.**
 
	 1. Public officials tend to implement policies in consistent sets, then. This can result in democratic deficits where the public fails to get the policies it wants -> e.g., high levels of support for non-punitive policies may fall on dear ears when the trends are for greater punitiveness -> when change does occur, though, public officials usually take notice because it means something has changed and cut through the usual stability.
	    
	 2. Many criminal justice actors in the USA are democratically elected and are thus exposed directly to the public mood -> seen many incumbents defeated because they were too soft (or sometimes in today's climate too harsh) on crime.
	    
	 3. However, even non-elected lifetime appointments have been shown to be responsive to the public mood concerning criminal justice issues.

### Main findings

1. Movements in observed criminal justice opinions in the USA have been gradual. Support for punitive policies was low in the 1960s and gradually increased until achieving record highs in the 1990s until declining to moderate levels in the 2010s.
   
2. Trends in criminal justice opinions have been parallel for different population groups.
   
3. There has been only one consistent predictor of changes in aggregate support for punitive policies: crime rates. The author is really cutting across the consensus in the field on this issue.
   
4. Most members of the public are uninformed about crime and criminal justice, but aggregate support for punitive policies move gradually in response to changes in the crime rate -> this relationship is mediated by changes in the number of news stories devoted to crime and the public's fear of crime.
   
5. Public opinion moves in response to crime rates and both criminal justice policy and practice respond to opinion movements.
	1. Why did it take so long for the public to become less punitive? #question 