---
type: [Article]
author: [Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan]
journal: [The Quarterly Journal of Economics]
date: 2018-02-01
---

* **Creation date**: `= this.file.ctime`
* **Last modified date**: `= this.file.mtime`

## Metadata

* **Author(s)**: Jon Kleinberg, Himabindu Lakkaraju, Jure Leskovec, Jens Ludwig, Sendhil Mullainathan
* **Title**: Human Decisions and Machine Predictions*
* **Date of publication**: 2018-02-01
* **Journal**: The Quarterly Journal of Economics
* **Volume**: 133
* **Issue**: 1
* **Pages**: 237-293
* **URL**: [https://doi.org/10.1093/qje/qjx032](https://doi.org/10.1093/qje/qjx032)
* **Tags**: #comps_exam, #computational_social_science, #human_machine_predictions, #sentencing_courts
* **PDF Attachments**:
  * [kleinbergHumanDecisionsMachine2018.pdf](zotero://open-pdf/library/items/UGHP297T)

## Abstract

Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals.

## My notes

Judges can identify low-risk cases pretty well and treat them accordingly. They have much more trouble identifying high-risk cases and then dealing with them accordingly.

### Research question

Can machine predictions aid human-decision making in complex domains e.g., in the decision to set bail or not?

* Not trying to improve machine predictions but human decisions.
  
* Only have outcomes for released defendants thus they develop an empirical approach to meaningfully compare human decisions to machine predictions.

### Data and Methods

* Around 758,000 defendants in NYC who were arrested between 2008 and 2013.
  
* Use an 80/20 split for train test. Use [[k-fold cross validation]] where k = 5 for [[hyperparameter tuning]].
	* They then include a super test set which is all arraigned cases in the last 6 months of data, and this super test set (also called the lock box) is not used until the paper is ready for publication.
	  
* Use [[gradient boosted decision trees]].

### High-level results

* They find that judge's decisions often do not comport with the algorithm's predictions, and the algorithm tends to be more accurate. This part of the paper is focused on predicting flight risk of defendants who judges decided to release (and thus their outcomes are actually observed). 

	* Rather than being inaccurate, perhaps judges set a high risk threshold for detention and (knowingly) release moderately-high risk defendants. How can this be evaluated since they only have judge decision making and not predictions of risk?
	
		* Suppose you have a lenient judge and a strict judge. The lenient judge sets a higher threshold for detention. If judges jail by descending risk, then stricter judges have a lower bar -> and jail the riskiest people released by the most lenient judge **first**.
	
		* Do judges' implicit rankings match the algorithm? They can answer this by looking at who they jail across levels of judge strictness.
		  
			* Stated more plainly:
			  
				* Suppose you had two judges: the strict judge who released 80% of defendants and the lenient judge who released 90% of defendants (conditional on observed variables).
				  
				* You have the algorithm predict risk for each judge's caseload.
				  
				* Then you can compare the distributions of predicted risk among the two judge's caseloads to determine where in the distribution the additional jailed peoples came from (for the stricter judge).
				  
				* In expectation, because assignment of individuals to judges is random, you would expect the two distributions to be balanced (on observed variable and unobserved variables). Thus, they are comparable. And then you would expect the stricter judge to pick from the riskiest part of the lenient judge's distribution that they did not jail.
		  
		* Results suggest judges are misranking defendants and not simply setting a threshold. Stricter judges do not simply jail the riskiest defendants -> marginal defendants are drawn from throughout the entire risk distribution (for strict, lenient, and in-between judges). In other words, the algorithm is well-calibrated (i.e., its predictions of risk map on to the real-world observed risk of failures to appear and recidivism). Judges are not well-calibrated.
	  
* These misrankings have large welfare reductions -> one could achieve the same crime rate by jailing fewer people **or** one could keep the same jailing rate and decrease the crime rate.
	  
	* They perform several counterfactuals to scope the size of potential gains which would result from specific policy changes.
	  
	* E.g., what would happen if all release decisions were based on predicted risk? This requires an evaluation of counterfactuals since some people who were jailed would get released and some people released would get jailed.
	  
		* How can you evaluate what the jailed people would have done? They predict outcomes for these defendants using the outcomes of similar defendants who were released (using [[train/test]]).
		
			* The as-good-as-random assignment procedure allows for labels to be created for individuals who were jailed thus creating the opportunity to predict risk for every individual. They also use a machine learning approach for this using the trained ML model to predict outcomes for defendants under the assumption that conditional on observed variables, the only difference between the two groups is the treatment (being assigned to a stricter judge or not thus leading to a higher chance of being jailed or not).
		  
		* What about [[omitted variable bias]]? Perhaps judges observe something relevant which the authors do not have data on. If judges are using the unobserved data well, those release may not be comparable to those who are not released.
		  
			* E.g., suppose judges see a gang tattoo (which only young people have) which is a known recidivism risk and thus are more likely to jail those individuals. The model, though, treats all young people the same and the only different between them is the treatment. Specifically, it would impute the crime risk of released young people to the jailed gang-member young people (and presumably this would be a severe understating of the risk posed by the individuals and thus overestimating the efficacy of the algorithm because the algorithm underestimates the amount of crime which would result from releasing these young gang members).
			  
			* Authors argue that the algorithm achieves most of its gains come from appropriately jailing high-risk individuals (who were released by judges) (which is something that can be observed directly) -> **and not from** releasing low-risk individuals (who judges jailed) (which is where the bias come in because we do not know what would have happened).
			  
			* Authors also perform a bounding exercise in which they assume that every person jailed would have committed a crime. They find that the algorithm still would produce large gains (because judges are bad at properly jailing high-risk individuals).
			  
		* Are judges really making mistakes this egregious? Perhaps judges have other objectives beyond the outcome which the algorithm is predicting.
		  
			* Perhaps judges are taking into account employment or family circumstances and ignoring criminal risk. #highlight #question #disagree The authors say their data does not allow them to evaluate this possibility. Using data from another experiment, though, they found the introduction of risk assessment changed judicial decision making. If judges were optimizing with respect to other criteria, you would not expect judges to change their behavior.
			  
				* This strikes me as naive since the introduction of risk assessments can change courtroom behaviors and objectives. I also doubt how much risk assessment will really change judge behavior.
				  
			* Account for the possibility that judges consider jail capacity constraints.
			* Train the algorithm in one time period and evaluate it in a later time period.
			* Account for judges having to also administer money bail which may be what is causing the mismatch with the algorithm.
			  
* The results suggest through following the algorithm, one could reduce jail populations and reduce racial disparities.
  
* Heterogeneity analysis suggests that all types of crimes fall in incidence rate (and not just certain types of crimes while other crimes increase in incidence rate creating an average negative effect that masks the heterogeneity by crime type).
  
* Why do judges mis-predict? They train a model to predict whether a judge will release a defendant (based on observable variables). One argument is when the judge deviates from the algorithm, the judge has access to information the information-deprived algorithm does not.
  
	* To test this, they build a release rule based on the simulated judge. They begin with a set of cases released by the most lenient judge (thus they observe what really happened as a result of these people being released). They then jail additional defendants according to the simulated judge's predicted risk (those seen as the most risky are jailed first). The simulated judge out-performs real judges in every leniency percentile (meaning these simulated judges are better able to identify the riskiest defendants released and instead appropriately jail them). The authors conclude whatever unobserved variables which cause judges to deviate from their *normal self* (e.g., internal states. specific case features. or defendant features) -> these variables create noise.
	  
* Results generalize outside of NYC. They run a similar procedure on 151,561 defendants between 1990 and 2009 in 40 large urban counties across the USA.
  
* Finally, they find that machine learning methods provided a sizable boost in prediction performance over [[Logistic regression]]. However, even logistic regression performed better than judges did. While machine learning is useful, the epistemological benefits are also important too (i.e., the kind of problem it causes researchers to focus on).